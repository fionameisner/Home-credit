{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the tools used in this notebook\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder,LabelEncoder\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('seaborn-darkgrid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# View data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ntest = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\ntrain.head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TARGET'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TARGET'].astype(int).plot.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the missing values in their own table. You can see that a few of the columns are missing a lot of their data.\nmissing_values = train.isnull().sum()\nmissing_values_percent = 100*train.isnull().sum()/len(train)\nmissing_val_table = pd.concat([missing_values,missing_values_percent],axis=1)\nmissing_values_col = missing_val_table.rename(columns={0:'Missing Value',1 :\"percent of value\"})\nmissing_values_col = missing_values_col[missing_val_table.iloc[:,1]!=0].sort_values(\"percent of value\",ascending=False).round(1)\nmissing_values_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\nle_count = 0\nfor col in train:\n  if train[col].dtype=='object':\n    if len(list(train[col].unique())) <=2:\n      le.fit(train[col])\n      train[col]=le.transform(train[col])\n      test[col]=le.transform(test[col])\n      le_count += 1\nprint('Encoding ------------- completed')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)\ntrain_labels = train['TARGET']\ntrain, test_dat = train.align(test, join='inner', axis=1)\ntrain['TARGET'] = train_labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\nplt.xlabel('Days Employment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def new_features(train,test):\n  \"\"\"Adding a few new features to the data\"\"\"\n  try:\n    train['DAYS_EMPLOYED_ANOM'] = train[\"DAYS_EMPLOYED\"] == 365243\n    train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n    train['ANNUITY_INCOME_PERCENT'] = train['AMT_ANNUITY'] / \\\n        train['AMT_INCOME_TOTAL']\n    train['CREDIT_TERM'] = train['AMT_ANNUITY'] / \\\n        train['AMT_CREDIT']\n    train['DAYS_EMPLOYED_PERCENT'] = train['DAYS_EMPLOYED'] / \\\n        train['DAYS_BIRTH']\n\n    test['DAYS_EMPLOYED_ANOM'] = test['DAYS_EMPLOYED'] == 365243\n    test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n\n    train['CREDIT_INCOME_PERCENT'] = train['AMT_CREDIT'] / \\\n        train['AMT_INCOME_TOTAL']\n    test['CREDIT_INCOME_PERCENT'] = test['AMT_CREDIT'] / \\\n        test['AMT_INCOME_TOTAL']\n    test['ANNUITY_INCOME_PERCENT'] = test['AMT_ANNUITY'] / \\\n        test['AMT_INCOME_TOTAL']\n    test['CREDIT_TERM'] = test['AMT_ANNUITY'] / \\\n        test['AMT_CREDIT']\n    test['DAYS_EMPLOYED_PERCENT'] = test['DAYS_EMPLOYED'] / \\\n        test['DAYS_BIRTH']\n    return train,test\n  except:\n    print(\"New feature function is not working, try again.\")\n  return train,test\nnew_features(train,test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations = train.corr()['TARGET'].sort_values()\ncorrelations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_age(age_days_negative):\n    age_days_positive = -age_days_negative\n    age_years = age_days_positive/365\n    return age_years\n\ntrain['DAYS_BIRTH'] = train['DAYS_BIRTH'].apply(change_age)\ntrain['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].apply(change_age)\n\ntest['DAYS_BIRTH'] = test['DAYS_BIRTH'].apply(change_age)\ntest['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].apply(change_age)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a grapgh for see the age of the client because DAYS_BIRTH has a high correlation.\nplt.style.use('ggplot')\nplt.figure(figsize=(10, 8))\nplt.title('Age Distribution')\nplt.xlabel('Age')\nsns.kdeplot(train[train['TARGET'] == 1]['DAYS_BIRTH'], label='Target=1')\nsns.kdeplot(train[train['TARGET'] == 0]['DAYS_BIRTH'], label='Target=0')\nplt.grid()\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a pipeline for numerical features\nnumeric_transformer = Pipeline(\n    steps=[(\"num_imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", MinMaxScaler())])\n# create a pipeline for catagorical features\ncategorical_transformer = Pipeline(steps=[(\"cat_imputer\", SimpleImputer(\n    strategy=\"most_frequent\")), (\"encoder\", OneHotEncoder(drop='first'))])\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n    (\"cat\", categorical_transformer, make_column_selector(dtype_include=\"object\"))])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split data","metadata":{}},{"cell_type":"code","source":"# Separate target\nX = train.drop('TARGET', axis=1)\ny = train['TARGET']\n\n# Splitting data\nX_train, x_test, y_train, y_test = train_test_split( X, y, train_size=0.8, stratify=y, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a function for trained models evaluation\ndef evaluate_model(model_pipeline):\n    # prediction\n    test_pred = model_pipeline.predict(x_test)\n    test_pred_proba = model_pipeline.predict_proba(x_test)\n\n    print('Validation roc auc score ---->   {:.4f}'.format(\n        roc_auc_score(y_test, test_pred_proba[:, 1])))\n\n    print('Confusion matrix:\\n', confusion_matrix(y_test, test_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create oversampler\nundersampler = RandomUnderSampler(sampling_strategy=0.75)\n# Create pipeline\nmodel = LGBMClassifier(n_estimators=100, num_leaves=36,class_weight='balanced'\n                      ,random_state=42, learning_rate=0.10)\nsteps = [('preprocessor', preprocessor),\n         ('undersampler', undersampler), ('model', model)]\nlgbm_pipeline = Pipeline(steps=steps)\n\nlgbm_pipeline.fit(X_train, y_train)\n\nevaluate_model(lgbm_pipeline)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"final_preds = lgbm_pipeline.predict(test)\nsubmission = pd.DataFrame(test['SK_ID_CURR'], columns=['SK_ID_CURR'])\nsubmission['TARGET'] = final_preds\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission_loan_light.csv', index=False, header=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}